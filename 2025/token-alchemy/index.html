<!doctype html><html lang=en-US><!doctype html><html><head><title>Token Alchemy</title>
<meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="llms killed my best friend"><meta name=keywords content="Technology,Code,Program,Linux,"><meta name=author content="Nick Karpov"><meta property="og:title" content="Token Alchemy"><meta property="og:description" content="Can you give an LLM a distribution as an input?"><meta property="og:type" content="website"><meta property="og:locale" content="en_US"><meta property="og:url" content="https://nickkarpov.com/2025/token-alchemy/"><meta property="og:image" content="https://nickkarpov.com/2025/token-alchemy/image.png"><meta property="article:published_time" content="2025-03-06T01:00:00Z"><link rel="shortcut icon" href=/favicon.ico><link rel=stylesheet href=/css/main.min.8a7c15ecafd33cb478712d698109fc39e0e0e2a4817fdb59d42f20484af07a08.css integrity="sha256-inwV7K/TPLR4cS1pgQn8OeDg4qSBf9tZ1C8gSErwegg="><script defer src="/javascript/fontawesome.min.be83f86daeb960ece02aec3f9b5915b790b371ffca334c3023d911997e2139f6.js" integrity="sha256-voP4ba65YOzgKuw/m1kVt5Czcf/KM0wwI9kRmX4hOfY="></script><link href=http://gmpg.org/xfn/11 rel=profile><meta name=generator content="Hugo 0.143.1"></head></html><body class="min-h-full max-w-4xl font-sans mx-4 lg:mx-auto xl:mx-auto leading-normal my-8"><header class="mx-auto pb-4"><h1 class="text-center lg:text-left text-4xl text-gray-900 font-medium m-0"><a class=text-gray-900 href=/>Nick Karpov</a></h1><div class="lg:flex text-center justify-between"><p class="text-lg text-gray-600">llms killed my best friend</p><nav class=my-auto><ul class="justify-center flex flex-wrap list-disc list-inside"><li class="list-none mx-2"><a class=text-gray-600 href=/index.xml><i class="text-2xl fas fa-rss"></i></a></li><li class="list-none mx-2"><a class=text-gray-600 href=https://x.com/nickkarpov><i class="text-2xl fas fa-twitter"></i></a></li></ul></nav></div><nav class="text-center border-t border-b px-1 mt-4"><ul class="list-disc list-inside"><li class="inline-block list-none mx-4"><a href=/about>About</a></li><li class="inline-block list-none mx-4"><a href=/archives/>Archives</a></li><li class="inline-block list-none mx-4"><a href=/categories/>Categories</a></li><li class="inline-block list-none mx-4"><a href=/tags/>Tags</a></li></ul></nav></header><main class="flex-1 text-gray-900"><article class="container pt-8"><header class="pb-4 mb-8 border-b"><h1 class="my-2 text-black leading-tight">Token Alchemy</h1></header><div class=post-content><blockquote><p>Dumb question but you know how LLMs output a distribution over tokens? Can you also give it a distribution as input?</p></blockquote><p>This question <a href=https://x.com/ja3k_/status/1895638074576814552>on X</a> caught my eye and I decided to explore it further.</p><p>Now, strictly speaking the answer is <strong>no</strong>, LLMs don't take a distribution as an input: LLMs take a discrete sequence of tokens as an input. They are created by splitting the input text ("tokenizing") into pieces ("tokens") and looking up the vector representation of each of those pieces in an embedding matrix.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>from</span> <span style=color:#0e84b5;font-weight:700>transformers</span> <span style=color:#007020;font-weight:700>import</span> AutoModelForCausalLM, AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#666>=</span> AutoTokenizer<span style=color:#666>.</span>from_pretrained(<span style=color:#4070a0>&#34;microsoft/phi-2&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>input_text <span style=color:#666>=</span> <span style=color:#4070a0>&#34;Dumb question&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokens <span style=color:#666>=</span> tokenizer(input_text, return_tensors<span style=color:#666>=</span><span style=color:#4070a0>&#34;pt&#34;</span>)
</span></span><span style=display:flex><span>embeddings <span style=color:#666>=</span> model<span style=color:#666>.</span>get_input_embeddings()(tokens[<span style=color:#4070a0>&#34;input_ids&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020>print</span>(tokens)
</span></span><span style=display:flex><span><span style=color:#007020>print</span>(embeddings)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic>## output </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># Dumb question = 3 tokens, [  35, 2178, 1808]</span>
</span></span><span style=display:flex><span>{<span style=color:#4070a0>&#39;input_ids&#39;</span>: tensor([[  <span style=color:#40a070>35</span>, <span style=color:#40a070>2178</span>, <span style=color:#40a070>1808</span>]]), <span style=color:#4070a0>&#39;attention_mask&#39;</span>: tensor([[<span style=color:#40a070>1</span>, <span style=color:#40a070>1</span>, <span style=color:#40a070>1</span>]])}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># Each token is represented by a vector</span>
</span></span><span style=display:flex><span>tensor([[[ <span style=color:#40a070>1.5388e-02</span>, <span style=color:#666>-</span><span style=color:#40a070>5.5420e-02</span>,  <span style=color:#40a070>1.3306e-02</span>,  <span style=color:#666>...</span>, <span style=color:#666>-</span><span style=color:#40a070>1.4153e-02</span>,
</span></span><span style=display:flex><span>           <span style=color:#40a070>1.7975e-02</span>,  <span style=color:#40a070>1.7044e-02</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#40a070>3.9215e-03</span>,  <span style=color:#40a070>2.7733e-03</span>,  <span style=color:#40a070>3.5736e-02</span>,  <span style=color:#666>...</span>,  <span style=color:#40a070>3.4088e-02</span>,
</span></span><span style=display:flex><span>           <span style=color:#40a070>5.6534e-03</span>, <span style=color:#666>-</span><span style=color:#40a070>7.3547e-02</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#40a070>1.0864e-02</span>, <span style=color:#666>-</span><span style=color:#40a070>4.7424e-02</span>, <span style=color:#666>-</span><span style=color:#40a070>9.4452e-03</span>,  <span style=color:#666>...</span>, <span style=color:#666>-</span><span style=color:#40a070>2.4048e-02</span>,
</span></span><span style=display:flex><span>          <span style=color:#666>-</span><span style=color:#40a070>3.8862e-05</span>, <span style=color:#666>-</span><span style=color:#40a070>1.5249e-03</span>]]], grad_fn<span style=color:#666>=&lt;</span>EmbeddingBackward0<span style=color:#666>&gt;</span>)
</span></span></code></pre></div><p>But seeing the tokenized input translated into the embedding (that we eventually pass to the LLM) reveals something interesting. Although the embeddings <strong>are a discrete and fixed size</strong>, the value of each dimension within the embedding <strong>is a continuous value</strong>.</p><p>Continuous values are a little more intuitive to work with when looking back at the original question, "can you give a distribution as an input". Let's see if if we can exploit these values to some use</p><h2 id=looks-similar>Looks similar...</h2><p>The most common way to exploit embeddings is to find other embeddings using various distance metrics. Different ways of interpreting the distance or difference between the embedding vectors hopefully translates into semantically (human) meaningful results.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>import</span> <span style=color:#0e84b5;font-weight:700>torch</span><span style=color:#666>,</span> <span style=color:#0e84b5;font-weight:700>torch.nn.functional</span> <span style=color:#007020;font-weight:700>as</span> <span style=color:#0e84b5;font-weight:700>F</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#666>=</span> AutoTokenizer<span style=color:#666>.</span>from_pretrained(<span style=color:#4070a0>&#34;microsoft/phi-2&#34;</span>)
</span></span><span style=display:flex><span>model <span style=color:#666>=</span> AutoModelForCausalLM<span style=color:#666>.</span>from_pretrained(<span style=color:#4070a0>&#34;microsoft/phi-2&#34;</span>)<span style=color:#666>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># get the embedding matrix</span>
</span></span><span style=display:flex><span>W <span style=color:#666>=</span> model<span style=color:#666>.</span>get_input_embeddings()<span style=color:#666>.</span>weight
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>def</span> <span style=color:#06287e>top_bottom_sim</span>(word, k<span style=color:#666>=</span><span style=color:#40a070>5</span>):
</span></span><span style=display:flex><span>    i <span style=color:#666>=</span> tokenizer<span style=color:#666>.</span>encode(word, add_special_tokens<span style=color:#666>=</span><span style=color:#007020;font-weight:700>False</span>)[<span style=color:#40a070>0</span>]
</span></span><span style=display:flex><span>     <span style=color:#60a0b0;font-style:italic># get similarity to all other embeddings</span>
</span></span><span style=display:flex><span>    s <span style=color:#666>=</span> F<span style=color:#666>.</span>cosine_similarity(W[i]<span style=color:#666>.</span>unsqueeze(<span style=color:#40a070>0</span>), W, dim<span style=color:#666>=</span><span style=color:#40a070>1</span>)
</span></span><span style=display:flex><span>    topv, topi <span style=color:#666>=</span> torch<span style=color:#666>.</span>topk(s, k<span style=color:#666>+</span><span style=color:#40a070>1</span>) <span style=color:#60a0b0;font-style:italic># pick the most similar</span>
</span></span><span style=display:flex><span>    botv, boti <span style=color:#666>=</span> torch<span style=color:#666>.</span>topk(<span style=color:#666>-</span>s, k) <span style=color:#60a0b0;font-style:italic># pick the least similar</span>
</span></span><span style=display:flex><span>    <span style=color:#007020>print</span>(<span style=color:#4070a0>&#34;Top:&#34;</span>, [(tokenizer<span style=color:#666>.</span>decode([ix<span style=color:#666>.</span>item()]), <span style=color:#007020>round</span>(val<span style=color:#666>.</span>item(),<span style=color:#40a070>3</span>)) 
</span></span><span style=display:flex><span>                   <span style=color:#007020;font-weight:700>for</span> val, ix <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>zip</span>(topv[<span style=color:#40a070>1</span>:], topi[<span style=color:#40a070>1</span>:])])
</span></span><span style=display:flex><span>    <span style=color:#007020>print</span>(<span style=color:#4070a0>&#34;Bottom:&#34;</span>, [(tokenizer<span style=color:#666>.</span>decode([ix<span style=color:#666>.</span>item()]), <span style=color:#007020>round</span>(<span style=color:#666>-</span>val<span style=color:#666>.</span>item(),<span style=color:#40a070>3</span>)) 
</span></span><span style=display:flex><span>                      <span style=color:#007020;font-weight:700>for</span> val, ix <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>zip</span>(botv, boti)])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>top_bottom_sim(<span style=color:#4070a0>&#34;question&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic>## output</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Top: [(<span style=color:#4070a0>&#39; question&#39;</span>, <span style=color:#40a070>0.187</span>), (<span style=color:#4070a0>&#39; Question&#39;</span>, <span style=color:#40a070>0.161</span>), (<span style=color:#4070a0>&#39;Question&#39;</span>, <span style=color:#40a070>0.15</span>), (<span style=color:#4070a0>&#39;problem&#39;</span>, <span style=color:#40a070>0.139</span>), (<span style=color:#4070a0>&#39; questions&#39;</span>, <span style=color:#40a070>0.138</span>)]
</span></span><span style=display:flex><span>Bottom: [(<span style=color:#4070a0>&#39; Ember&#39;</span>, <span style=color:#666>-</span><span style=color:#40a070>0.082</span>), (<span style=color:#4070a0>&#39; fetal&#39;</span>, <span style=color:#666>-</span><span style=color:#40a070>0.081</span>), (<span style=color:#4070a0>&#39; Union&#39;</span>, <span style=color:#666>-</span><span style=color:#40a070>0.08</span>), (<span style=color:#4070a0>&#39; Moss&#39;</span>, <span style=color:#666>-</span><span style=color:#40a070>0.076</span>), (<span style=color:#4070a0>&#39; expired&#39;</span>, <span style=color:#666>-</span><span style=color:#40a070>0.074</span>)]
</span></span></code></pre></div><p>The code above is a simple example to find 5 most similar and dissimilar tokens to "question". Here "similarity" is defined as the <a href=https://en.wikipedia.org/wiki/Cosine_similarity>cosine similarity</a>. I like to think of it as the angle between vectors: the smaller the angle the more "similar" the vectors.</p><p>There's many ways to derive these embedding matrices to position tokens in a space that allow for these kinds of similarity measurements. Very roughly the "reason" this works is that "similar" words are surrounded by other "similar" words in training data. That is to say, the token <strong>"question"</strong>, tends to appear in the same places as <strong>"Question"</strong> (capitalization matters!), and, apparently, around the same places as <strong>"problem"</strong> (which also probably makes intuitive sense). The <a href=https://en.wikipedia.org/wiki/Word2vec>Word2Vec</a> algorithm and paper is a great read for this.</p><h2 id=what-does-this-have-to-do-with-distribution-as-input>What does this have to do with distribution as input?</h2><p>Returning to the original question, can you give an LLM a distribution as an input?</p><p>Although the answer is ~~no, we do see that the vector representation of the input tokens are something we can manipulate. Above we've used the representations for the most basic usage, similarity, but we can also directly multiply, add, or blend these vectors together <strong>and pass those</strong> as input to the LLM instead.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># Blend each token with the average of its 5 nearest neighbors</span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>def</span> <span style=color:#06287e>blend_tokens</span>(tokens):
</span></span><span style=display:flex><span>    blended <span style=color:#666>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> tid <span style=color:#007020;font-weight:700>in</span> tokens:
</span></span><span style=display:flex><span>        sims <span style=color:#666>=</span> F<span style=color:#666>.</span>cosine_similarity(W[tid]<span style=color:#666>.</span>unsqueeze(<span style=color:#40a070>0</span>), W, dim<span style=color:#666>=</span><span style=color:#40a070>1</span>)
</span></span><span style=display:flex><span>        _, top_idx <span style=color:#666>=</span> torch<span style=color:#666>.</span>topk(sims, <span style=color:#40a070>6</span>)  <span style=color:#60a0b0;font-style:italic># token itself + 5 neighbors</span>
</span></span><span style=display:flex><span>        blended<span style=color:#666>.</span>append(W[top_idx[<span style=color:#40a070>1</span>:]]<span style=color:#666>.</span>mean(dim<span style=color:#666>=</span><span style=color:#40a070>0</span>))
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span> torch<span style=color:#666>.</span>stack(blended)<span style=color:#666>.</span>unsqueeze(<span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># Example</span>
</span></span><span style=display:flex><span>text <span style=color:#666>=</span> <span style=color:#4070a0>&#34;Dumb question here&#34;</span>
</span></span><span style=display:flex><span>input_ids <span style=color:#666>=</span> tokenizer<span style=color:#666>.</span>encode(text, return_tensors<span style=color:#666>=</span><span style=color:#4070a0>&#34;pt&#34;</span>)<span style=color:#666>.</span>to(model<span style=color:#666>.</span>device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># Generate from original</span>
</span></span><span style=display:flex><span>original_output <span style=color:#666>=</span> model<span style=color:#666>.</span>generate(input_ids, max_new_tokens<span style=color:#666>=</span><span style=color:#40a070>10</span>, pad_token_id<span style=color:#666>=</span>tokenizer<span style=color:#666>.</span>eos_token_id)
</span></span><span style=display:flex><span><span style=color:#007020>print</span>(<span style=color:#4070a0>&#34;</span><span style=color:#4070a0;font-weight:700>\n</span><span style=color:#4070a0>Original: &#34;</span>, tokenizer<span style=color:#666>.</span>decode(original_output[<span style=color:#40a070>0</span>]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># Generate from blended</span>
</span></span><span style=display:flex><span>blended_emb <span style=color:#666>=</span> blend_tokens(input_ids[<span style=color:#40a070>0</span>])<span style=color:#666>.</span>to(model<span style=color:#666>.</span>device)
</span></span><span style=display:flex><span>blended_output <span style=color:#666>=</span> model<span style=color:#666>.</span>generate(inputs_embeds<span style=color:#666>=</span>blended_emb, max_new_tokens<span style=color:#666>=</span><span style=color:#40a070>10</span>, pad_token_id<span style=color:#666>=</span>tokenizer<span style=color:#666>.</span>eos_token_id)
</span></span><span style=display:flex><span><span style=color:#007020>print</span>(<span style=color:#4070a0>&#34;</span><span style=color:#4070a0;font-weight:700>\n</span><span style=color:#4070a0>Blended:&#34;</span>, tokenizer<span style=color:#666>.</span>decode(blended_output[<span style=color:#40a070>0</span>]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic>## output</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Original: Dumb question here, but I<span style=color:#4070a0>&#39;m not sure how to do it</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Blended: Dumb question here<span style=color:#666>.</span> I<span style=color:#4070a0>&#39;m not sure what you mean by</span>
</span></span></code></pre></div><p>We can see in the output from the blended example is not the same as the original input, which demonstrates that the LLM can still yield interesting results after manipulating the input embeddings that don't have an exact "meaning".</p><p><code>blend_tokens</code> averages each of the input tokens with the 5 most "similar" tokens to them. If we were to look these up in our embedding matrix it wouldn't translate to an explicit token.</p><p>This is how we move towards a "distribution as an input": by doing some preprocessing on the input vectors before passing them to the LLM.</p><h2 id=now-what>Now what?</h2><p>I'm actually not sure. I just stumbled into this because of the original question. I think a next step could be to explore potential ways to manipulate the input embeddings, we only tried averaging here, which probably isn't very meaningful.</p><p>Another thing is that I don't think API providers allow raw vector inputs, they tokenize the input text themselves, so we couldn't do this experiment with those.</p><p>It's possible we could use this technique to "find" other tokens that might have interesting output, and pass those instead to the API providers. This sounds a little like adversarial input research related... I suppose that's an exercise left for the reader.</p></div><div class="text-center pt-4 grid grid-cols-12"></div><hr class="border-gray-200 mt-10 mb-4"><div class=giscus></div><script src=https://giscus.app/client.js data-repo=Xuanwo/blog data-repo-id="MDEwOlJlcG9zaXRvcnkyMDYwNzE1Nw==" data-category=Announcements data-category-id=DIC_kwDOATpwtc4B-a6u data-mapping=title data-reactions-enabled=1 data-emit-metadata=0 data-theme=light crossorigin=anonymous async></script></article></main></body></html>